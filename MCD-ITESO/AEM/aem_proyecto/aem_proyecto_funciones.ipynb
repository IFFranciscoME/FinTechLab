{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook: Funciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://oci02.img.iteso.mx/identidad_de_instancia_2018/ITESO/Logos%20ITESO/Logo-ITESO-Principal.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <font color= #004A94> <font size = 6> Msc Ciencia de datos </font> <br> <br> <font color= #047CFB> <font size = 4>I.F. Juan Francisco Muñoz Elguezabal - franciscome@iteso.mx </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font color= #004A94> <font size = 6> ANÁLISIS ESTADÍSTICO MULTIVARIABLE </font> <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font color=#42c29b><font size=5> Una comparación entre modelos regresivos lineales y clustering secuencial como predictores de series de tiempo financieras </font> <br> <br>\n",
    "\n",
    "<center><font color=#047CFB><font size=5> El caso  de una estrategia de trading para el mercado internacional de divisas </font> <br> <br> \n",
    "    \n",
    "<center> <font color= #047CFB> <font size = 4> Repositorio: <a href='https://github.com/IFFranciscoME/FinTechLab/tree/master/'>Link</a></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#004A94\"> Cargar librerías </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                     # funciones numericas\n",
    "import pandas as pd                                    # dataframes y utilidades\n",
    "from statsmodels.tsa.api import acf, pacf              # funciones de econometria\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler       # estandarizacion de variables\n",
    "from sklearn.decomposition import PCA                  # analisis de componentes principales (PCA)\n",
    "import statsmodels.api as sm                           # utilidades para modelo regresion lineal\n",
    "from sklearn.model_selection import train_test_split   # separacion de conjunto de entrenamiento y prueba\n",
    "\n",
    "pd.set_option('display.max_rows', None)                # sin limite de renglones maximos para mostrar pandas\n",
    "pd.set_option('display.max_columns', None)             # sin limite de columnas maximas para mostrar pandas\n",
    "pd.set_option('display.width', None)                   # sin limite el ancho del display\n",
    "pd.set_option('display.expand_frame_repr', False)      # visualizar todas las columnas de un dataframe\n",
    "pd.options.mode.chained_assignment = None              # para evitar el warning enfadoso de indexacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#004A94\"> FUNCIÓN: Generación de variables exógenas </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#004A94\"> FUNCIÓN: Generación de variables endógenas </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se hizo que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- --------------------------------------- FUNCION: Generacion de variables ENDOGENAS series de tiempo -- #\n",
    "# -- ------------------------------------------------------------------------------------ Version manual -- #\n",
    "\n",
    "def f_features_end(p_datos):\n",
    "    \"\"\"\n",
    "    :param p_datos: pd.DataFrae : dataframe con 5 columnas 'timestamp', 'open', 'high', 'low', 'close'\n",
    "        :return: r_features : dataframe con 5 columnas, nombres cohercionados + Features generados\n",
    "\n",
    "    # Debuging\n",
    "    p_datos = df_precios\n",
    "    p_datos = pd.DataFrame({''timestamp': {}, 'open': np.random.normal(1.1400, 0.0050, 20).\n",
    "                                              'high': np.random.normal(1.1400, 0.0050, 20),\n",
    "                                              'low': np.random.normal(1.1400, 0.0050, 20),\n",
    "                                              'close': np.random.normal(1.1400, 0.0050, 20)})\n",
    "    \"\"\"\n",
    "\n",
    "    datos = p_datos\n",
    "    datos.columns = ['timestamp', 'open', 'high', 'low', 'close']\n",
    "\n",
    "    cols = list(datos.columns)[1:]\n",
    "    datos[cols] = datos[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # formato columna timestamp como 'datetime'\n",
    "    datos['timestamp'] = pd.to_datetime(datos['timestamp'])\n",
    "    # datos['timestamp'] = datos['timestamp'].dt.tz_localize('UTC')\n",
    "\n",
    "    # rendimiento logaritmico de ventana 1\n",
    "    datos['logrend'] = np.log(datos['close']/datos['close'].shift(1)).dropna()\n",
    "\n",
    "    # pips descontados al cierre\n",
    "    datos['co'] = (datos['close']-datos['open'])*10000\n",
    "\n",
    "    # pips descontados alcistas\n",
    "    datos['ho'] = (datos['high'] - datos['open'])*10000\n",
    "\n",
    "    # pips descontados bajistas\n",
    "    datos['ol'] = (datos['open'] - datos['low'])*10000\n",
    "\n",
    "    # pips descontados en total (medida de volatilidad)\n",
    "    datos['hl'] = (datos['high'] - datos['low'])*10000\n",
    "\n",
    "    # funciones de ACF y PACF para determinar ancho de ventana historica\n",
    "    data_acf = acf(datos['logrend'].dropna(), nlags=12, fft=True)\n",
    "    data_pac = pacf(datos['logrend'].dropna(), nlags=12)\n",
    "    sig = round(1.96/np.sqrt(len(datos['logrend'])), 4)\n",
    "\n",
    "    # componentes AR y MA\n",
    "    maxs = list(set(list(np.where((data_pac > sig) | (data_pac < -sig))[0]) +\n",
    "                    list(np.where((data_acf > sig) | (data_acf < -sig))[0])))\n",
    "    # encontrar la componente maxima como indicativo de informacion historica autorelacionada\n",
    "    max_n = maxs[np.argmax(maxs)]\n",
    "\n",
    "    # condicion arbitraria: 5 resagos minimos para calcular variables moviles\n",
    "    if max_n <= 2:\n",
    "        max_n = 5\n",
    "\n",
    "    # ciclo para calcular N features con logica de \"Ventanas de tamaño n\"\n",
    "    for n in range(0, max_n):\n",
    "\n",
    "        # resago n de ho\n",
    "        datos['lag_ho_' + str(n + 1)] = np.log(datos['ho'].shift(n + 1))\n",
    "\n",
    "        # resago n de ol\n",
    "        datos['lag_ol_' + str(n + 1)] = np.log(datos['ol'].shift(n + 1))\n",
    "\n",
    "        # promedio movil de ventana n\n",
    "        datos['ma_ol_' + str(n + 2)] = datos['ol'].rolling(n + 2).mean()\n",
    "\n",
    "        # promedio movil de ventana n\n",
    "        datos['ma_ho_' + str(n + 2)] = datos['ho'].rolling(n + 2).mean()\n",
    "\n",
    "    # asignar timestamp como index\n",
    "    datos.index = pd.to_datetime(datos['timestamp'])\n",
    "    # quitar columnas no necesarias para modelos de ML\n",
    "    datos = datos.drop(['timestamp', 'open', 'high', 'low', 'close', 'hl', 'logrend'], axis=1)\n",
    "    # borrar columnas donde exista solo NAs\n",
    "    r_features = datos.dropna(axis='columns', how='all')\n",
    "    # borrar renglones donde exista algun NA\n",
    "    r_features = r_features.dropna(axis='rows')\n",
    "    # convertir a numeros tipo float las columnas\n",
    "    r_features.iloc[:, 1:] = r_features.iloc[:, 1:].astype(float)\n",
    "    # estandarizacion de todas las variables independientes\n",
    "    lista = r_features[list(r_features.columns[1:])]\n",
    "    r_features[list(r_features.columns[1:])] = StandardScaler().fit_transform(lista)\n",
    "\n",
    "    return r_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#004A94\"> FUNCIÓN: Ajuste de Regresión Lineal Multivariada </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- ---------------------------------------------------------------------- FUNCION: Ajustar RLM a datos -- #\n",
    "# -- ---------------------------------------------------------------------- ---------------------------- -- #\n",
    "\n",
    "def f_rlm(p_datos, p_y):\n",
    "    \"\"\"\n",
    "    :param p_datos: pd.DataFrame : DataFrame con variable \"y\" (1era col), y n variables \"x_n\" (2:n)\n",
    "    :param p_y : str : nombre de la columna a elegir como variable dependiente Y\n",
    "    :return:\n",
    "    p_datos = df_datos\n",
    "    \"\"\"\n",
    "\n",
    "    datos = p_datos\n",
    "\n",
    "    # Reacomodar los datos como arreglos\n",
    "    y_multiple = np.array(datos[p_y])\n",
    "    x_multiple = np.array(datos.iloc[:, 1:])\n",
    "\n",
    "    # datos para entrenamiento y prueba\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x_multiple, y_multiple,\n",
    "                                                        test_size=0.8, shuffle=False)\n",
    "\n",
    "    # Agregar interceptos a X en entrenamiento y prueba\n",
    "    train_x_betha = sm.add_constant(train_x)\n",
    "    test_x_betha = sm.add_constant(test_x)\n",
    "\n",
    "    # Modelo ajustado (entrenamiento)\n",
    "    modelo_train = sm.OLS(train_y, train_x_betha)\n",
    "    # Resultados de ajuste de modelo (entrenamiento)\n",
    "    modelo_fit_train = modelo_train.fit()\n",
    "\n",
    "    # Modelo ajustado (prueba)\n",
    "    modelo_test = sm.OLS(test_y, test_x_betha)\n",
    "    # Resultados de ajuste de modelo (prueba)\n",
    "    modelo_fit_test = modelo_test.fit()\n",
    "\n",
    "    # -- Con datos de ENTRENAMIENTO\n",
    "    # modelo completo resultante\n",
    "    r_train_modelo = modelo_fit_train\n",
    "    # summary de resultados del modelo\n",
    "    r_train_summary = r_train_modelo.summary()\n",
    "    # DataFrame con nombre de parametros segun dataset, nombre de parametros y pvalues segun modelo\n",
    "    r_df_train = pd.DataFrame({'df_params': ['intercepto'] + list(datos.columns[1:]),\n",
    "                               'm_params': r_train_modelo.model.data.param_names,\n",
    "                               'pv_params': r_train_modelo.pvalues})\n",
    "    # valor de AIC del modelo\n",
    "    r_train_aic = r_train_modelo.aic\n",
    "    # valor de BIC del modelo\n",
    "    r_train_bic = r_train_modelo.bic\n",
    "\n",
    "    # -- Con datos de PRUEBA\n",
    "    # modelo completo resultante\n",
    "    r_test_modelo = modelo_fit_test\n",
    "    # summary de resultados del modelo\n",
    "    r_test_summary = r_test_modelo.summary()\n",
    "    # DataFrame con nombre de parametros segun dataset, nombre de parametros y pvalues segun modelo\n",
    "    r_df_test = pd.DataFrame({'df_params': ['intercepto'] + list(datos.columns[1:]),\n",
    "                              'm_params': r_test_modelo.model.data.param_names,\n",
    "                              'pv_params': r_test_modelo.pvalues})\n",
    "    # valor de AIC del modelo\n",
    "    r_test_aic = r_test_modelo.aic\n",
    "    # valor de BIC del modelo\n",
    "    r_test_bic = r_test_modelo.bic\n",
    "\n",
    "    # tabla de resultados periodo de entrenamiento\n",
    "    r_df_pred_train = pd.DataFrame({'y': train_y, 'y_ajustada': modelo_fit_train.predict()})\n",
    "    # tabla de resultados periodo de prueba\n",
    "    r_df_pred_test = pd.DataFrame({'y': test_y, 'y_ajustada': modelo_fit_test.predict()})\n",
    "\n",
    "    r_d_modelo = {'train': {'modelo': r_train_modelo, 'summary': r_train_summary, 'parametros': r_df_train,\n",
    "                            'resultado': r_df_pred_train, 'aic': r_train_aic, 'bic': r_train_bic},\n",
    "                  'test': {'modelo': r_test_modelo, 'summary': r_test_summary, 'parametros': r_df_test,\n",
    "                            'resultado': r_df_pred_test, 'aic': r_test_aic, 'bic': r_test_bic}}\n",
    "\n",
    "    return r_d_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#004A94\"> FUNCIÓN: Reducción de dimensionalidad con PCA </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ---------------------------------------------------------------------- FUNCION: Aplicar PCA a datos -- #\n",
    "# -- ---------------------------------------------------------------------- ---------------------------- -- #\n",
    "\n",
    "def f_pca(p_datos, p_exp):\n",
    "    \"\"\"\n",
    "    :param p_datos:\n",
    "    :param p_exp:\n",
    "    :return:\n",
    "\n",
    "    p_datos = df_datos\n",
    "    p_exp = .90\n",
    "    \"\"\"\n",
    "    datos = p_datos\n",
    "\n",
    "    pca = PCA(n_components=10)\n",
    "    datos_pca = datos.iloc[:, 1:]\n",
    "    pca.fit(datos_pca)\n",
    "    # Calcular los vectores y valores propios de la martiz de covarianza\n",
    "    w, v = np.linalg.eig(pca.get_covariance())\n",
    "    # ordenar los valores de mayor a menor\n",
    "    indx = np.argsort(w)[::-1]\n",
    "    # calcular el procentaje de varianza en cada componente\n",
    "    porcentaje = w[indx] / np.sum(w)\n",
    "    # calcular el porcentaje acumulado de los componentes\n",
    "    porcent_acum = np.cumsum(porcentaje)\n",
    "    # encontrar las componentes necesarias para lograr explicar el 90% de variabilidad\n",
    "    pca_90 = np.where(porcent_acum > p_exp)[0][0] + 1\n",
    "\n",
    "    pca = PCA(n_components=pca_90)\n",
    "    datos_pca = datos.iloc[:, 1:]\n",
    "    df1 = datos.iloc[:, 0]\n",
    "    pca.fit(datos_pca)\n",
    "    df2 = pd.DataFrame(pca.transform(datos_pca))\n",
    "\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    r_datos_pca = pd.concat([df1, df2], axis=1)\n",
    "    r_datos_pca.index = datos_pca.index\n",
    "\n",
    "    # Renombrar columnas\n",
    "    r_datos_pca.columns = ['pca_y'] + ['pca_x_' + str(i) for i in range(0, pca_90)]\n",
    "\n",
    "    return r_datos_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
